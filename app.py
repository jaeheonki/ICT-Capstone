# -*- coding: utf-8 -*-
"""app1.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YOaNasrUl4zzeHdBydHC3btjPjAXgbwl
"""


import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
#transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

#GPU 사용
device = torch.device("cuda:0")

import inspect
import pandas as pd
import re
import os
from torch.optim import AdamW
import gc
import matplotlib.pyplot as plt
import seaborn as sns 

import missingno as msno 

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from konlpy.tag import Hannanum

from collections import Counter
import nltk
from nltk.corpus import wordnet
import torch
import math
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

import streamlit as st

st.title("DR.Kim GPT")





st.header("텍스트를 이곳에 붙여넣기 해주세요")
text_input = st.text_area("텍스트 입력", height=400)
button = st.button("입력")

if button:
    with st.spinner('Processing...'):
        chatbot_data = pd.read_csv('per0 + gpt1 + feature final.csv')
        
        chatbot_data.loc[(chatbot_data['label'] == "0"), 'label'] = 0  #per => 0
        chatbot_data.loc[(chatbot_data['label'] == "1"), 'label'] = 1  #gpt => 1
        
        
        data_list = []
        for q, label in zip(chatbot_data['text'], chatbot_data['label'])  :
            data = []
            data.append(q)
            data.append(str(label))
        
            data_list.append(data)
            
        #train & test 데이터로 나누기
        from sklearn.model_selection import train_test_split
                                                                 
        dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)
        
        # Setting parameters
        max_len = 250
        batch_size = 8
        warmup_ratio = 0.1
        num_epochs = 1
        max_grad_norm = 1
        log_interval = 200
        learning_rate =  5e-5
        
        class newBERTSentenceTransform(nlp.data.BERTSentenceTransform):
          r"""BERT style data transformation.
        
            Parameters
            ----------
            tokenizer : BERTTokenizer.
                Tokenizer for the sentences.
            max_seq_length : int.
                Maximum sequence length of the sentences.
            pad : bool, default True
                Whether to pad the sentences to maximum length.
            pair : bool, default True
                Whether to transform sentences or sentence pairs.
            """
            
          def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):
                self._tokenizer = tokenizer
                self._max_seq_length = max_seq_length
                self._pad = pad
                self._pair = pair
                self._vocab = vocab 
        
          def __call__(self, line):
                """Perform transformation for sequence pairs or single sequences.
        
                The transformation is processed in the following steps:
                - tokenize the input sequences
                - insert [CLS], [SEP] as necessary
                - generate type ids to indicate whether a token belongs to the first
                sequence or the second sequence.
                - generate valid length
        
                For sequence pairs, the input is a tuple of 2 strings:
                text_a, text_b.
        
                Inputs:
                    text_a: 'is this jacksonville ?'
                    text_b: 'no it is not'
                Tokenization:
                    text_a: 'is this jack ##son ##ville ?'
                    text_b: 'no it is not .'
                Processed:
                    tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'
                    type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
                    valid_length: 14
        
                For single sequences, the input is a tuple of single string:
                text_a.
        
                Inputs:
                    text_a: 'the dog is hairy .'
                Tokenization:
                    text_a: 'the dog is hairy .'
                Processed:
                    text_a: '[CLS] the dog is hairy . [SEP]'
                    type_ids: 0     0   0   0  0     0 0
                    valid_length: 7
        
                Parameters
                ----------
                line: tuple of str
                    Input strings. For sequence pairs, the input is a tuple of 2 strings:
                    (text_a, text_b). For single sequences, the input is a tuple of single
                    string: (text_a,).
        
                Returns
                -------
                np.array: input token ids in 'int32', shape (batch_size, seq_length)
                np.array: valid length in 'int32', shape (batch_size,)
                np.array: input token type ids in 'int32', shape (batch_size, seq_length)
        
                """
        
                # convert to unicode
                text_a = line[0]
                if self._pair:
                    assert len(line) == 2
                    text_b = line[1]
        
                tokens_a = self._tokenizer.tokenize(text_a)
        
                tokens_b = None
        
                if self._pair:
                    tokens_b = self._tokenizer(text_b)
        
                if tokens_b:
                    # Modifies `tokens_a` and `tokens_b` in place so that the total
                    # length is less than the specified length.
                    # Account for [CLS], [SEP], [SEP] with "- 3"
                    self._truncate_seq_pair(tokens_a, tokens_b,
                                            self._max_seq_length - 3)
                else:
                    # Account for [CLS] and [SEP] with "- 2"
                    if len(tokens_a) > self._max_seq_length - 2:
                        tokens_a = tokens_a[0:(self._max_seq_length - 2)]
        
                # The embedding vectors for `type=0` and `type=1` were learned during
                # pre-training and are added to the wordpiece embedding vector
                # (and position vector). This is not *strictly* necessary since
                # the [SEP] token unambiguously separates the sequences, but it makes
                # it easier for the model to learn the concept of sequences.
        
                # For classification tasks, the first vector (corresponding to [CLS]) is
                # used as as the "sentence vector". Note that this only makes sense because
                # the entire model is fine-tuned.
                #vocab = self._tokenizer.vocab
                vocab = self._vocab
                tokens = []
                tokens.append(vocab.cls_token)
                tokens.extend(tokens_a)
                tokens.append(vocab.sep_token)
                segment_ids = [0] * len(tokens)
        
                if tokens_b:
                    tokens.extend(tokens_b)
                    tokens.append(vocab.sep_token)
                    segment_ids.extend([1] * (len(tokens) - len(segment_ids)))
        
                input_ids = self._tokenizer.convert_tokens_to_ids(tokens)
        
                # The valid length of sentences. Only real  tokens are attended to.
                valid_length = len(input_ids)
        
                if self._pad:
                    # Zero-pad up to the sequence length.
                    padding_length = self._max_seq_length - valid_length
                    # use padding tokens for the rest
                    input_ids.extend([vocab[vocab.padding_token]] * padding_length)
                    segment_ids.extend([0] * padding_length)
        
                return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\
                    np.array(segment_ids, dtype='int32')
        
        class BERTDataset(Dataset):
            def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
                         pad, pair):
                transform = newBERTSentenceTransform(
                    bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)
        
                self.sentences = [transform([i[sent_idx]]) for i in dataset]
                self.labels = [np.int32(i[label_idx]) for i in dataset]
        
            def __getitem__(self, i):
                return (self.sentences[i] + (self.labels[i], ))
        
            def __len__(self):
                return (len(self.labels))
                
        data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)
        data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)
        
        train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)
        test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)
        
        class BERTClassifier(nn.Module):
            def __init__(self,
                         bert,
                         hidden_size = 768,
                         num_classes=2,   ##클래스 수 조정##
                         dr_rate=None,
                         params=None):
                super(BERTClassifier, self).__init__()
                self.bert = bert
                self.dr_rate = dr_rate
                         
                self.classifier = nn.Linear(hidden_size , num_classes)
                if dr_rate:
                    self.dropout = nn.Dropout(p=dr_rate)
            
            def gen_attention_mask(self, token_ids, valid_length):
                attention_mask = torch.zeros_like(token_ids)
                for i, v in enumerate(valid_length):
                    attention_mask[i][:v] = 1
                return attention_mask.float()
        
            def forward(self, token_ids, valid_length, segment_ids):
                attention_mask = self.gen_attention_mask(token_ids, valid_length)
                
                _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
                if self.dr_rate:
                    out = self.dropout(pooler)
                return self.classifier(out)
                
        os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        
        #BERT 모델 불러오기
        model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)
        
        #optimizer와 schedule 설정
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
        ]
        
        optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate) 
        loss_fn = nn.CrossEntropyLoss()
        
        t_total = len(train_dataloader) * num_epochs
        warmup_step = int(t_total * warmup_ratio)
        
        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)
        
        #정확도 측정을 위한 함수 정의
        def calc_accuracy(X,Y):
            max_vals, max_indices = torch.max(X, 1)
            train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
            return train_acc
          
        gc.collect()
        torch.cuda.empty_cache()
        
        for e in range(num_epochs):
            train_acc = 0.0
            test_acc = 0.0
            model.train()
            for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
                optimizer.zero_grad()
                token_ids = token_ids.long().to(device)
                segment_ids = segment_ids.long().to(device)
                valid_length= valid_length
                label = label.long().to(device)
                out = model(token_ids, valid_length, segment_ids)
                loss = loss_fn(out, label)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()
                scheduler.step()  # Update learning rate schedule
                train_acc += calc_accuracy(out, label)
                if batch_id % log_interval == 0:
                    print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
            print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
            
            model.eval()
            for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
                token_ids = token_ids.long().to(device)
                segment_ids = segment_ids.long().to(device)
                valid_length= valid_length
                label = label.long().to(device)
                out = model(token_ids, valid_length, segment_ids)
                test_acc += calc_accuracy(out, label)
            print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))
            
        #토큰화
        #tokenizer = get_tokenizer()
        #tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)
        classification = 0
        def sumsum(predict_sentence):
        
            data = [predict_sentence, '0']
            dataset_another = [data]
        
            another_test = BERTDataset(dataset_another, 0, 1, tokenizer,vocab, max_len, True, False)
            test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)
            
            model.eval()
        
            for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
                token_ids = token_ids.long().to(device)
                segment_ids = segment_ids.long().to(device)
        
                valid_length= valid_length
                label = label.long().to(device)
        
                out = model(token_ids, valid_length, segment_ids)
        
        
                test_eval=[]
                for i in out:
                    logits=i
                    logits = logits.detach().cpu().numpy()
        
                    if np.argmax(logits) == 0:
                        return 0
                    elif np.argmax(logits) == 1:
                        return 1
        
        data = pd.read_csv('final.csv')
        
        data = data.drop(columns = ['text'])
        y = data.label.values
        x_data = data.drop(['label'], axis = 1)
        
        x_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size = 0.2, random_state = 0)
        
        # Preprocess the data by standardizing it
        scaler = StandardScaler()
        x_train_scaled = scaler.fit_transform(x_train)
        x_test_scaled = scaler.transform(x_test)
        
        # Create an SVM classifier
        clf = svm.SVC(probability=True)
        
        # Train the SVM model
        clf.fit(x_train_scaled, y_train)
        
        # Evaluate the model on the test set
        accuracy = clf.score(x_test_scaled, y_test)
        
        text = text_input
        classification = sumsum(text)
        
        # Download the WordNet corpus if you haven't already
        nltk.download('wordnet')
        nltk.download('punkt')
        # Instantiate the Hannanum class
        hannanum = Hannanum()
        
        connecting_adverbs_list = ['아니면', '오직', '요컨데', '고로', '그러므로', '그런데', '그래서', '그리고', '혹은', '또는', '및', '그러나', '게다가', '더욱이', '더구나', '아울러', '뿐만 아니라', '동시에', '그런 점에서', '어쩌면', '하물며', '이처럼', '이같이', '바로', '하지만', '그렇지만', '그럼에도', '반면에', '도리어', '오히려', '반대로', '따라서', '그러니까', '그리하여', '그렇기 때문에', '그러면', '그러니', '급기야', '마침내', '왜냐하면', '다른 한편', '그렇기는 해도', '다만', '바꿔 말하면', '즉', '곧', '말하자면', '예를 들면', '일례로', '사실상', '예컨대', '덧붙여', '구체적으로', '왜냐하면', '이를테면', '다시 말하면', '끝으로', '결국', '결론적으로', '마지막으로', '요컨대', '결과적으로', '분명한 것은', '종합하면', '그런 다음']
        
        import os
        import random
        import tensorflow as tf
        from transformers import AutoTokenizer
        from transformers import TFGPT2LMHeadModel
        
        model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)
        tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')
        
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        qualifying_sentences = []
        
        
        pos_tags = hannanum.pos(text)
        
        # Count connecting adverbs
        connecting_adverb_counter = Counter()
        
        # Set for storing unique connecting adverbs
        unique_connecting_adverb_set = set()
        
        # Check for connecting adverbs using their POS tags
        for word, pos in pos_tags:
            if word in connecting_adverbs_list or pos == 'M':
                connecting_adverb_counter[word] += 1
                unique_connecting_adverb_set.add(word)
        
        # Calculate the average number of unique adverbs per sentence
        avg_adverbs_per_sentence = len(unique_connecting_adverb_set) / len(sentences)
        avg_adverbs_per_word = len(unique_connecting_adverb_set) / len(words)
        
        total_squared_diff = 0
        total_average = len(words) / len(sentences)
        
        # Calculate adjusted averages
        if len(unique_connecting_adverb_set) >= 5 and avg_adverbs_per_sentence >= 1.0:
            avg_adverbs_per_sentence += 0.3
            avg_adverbs_per_word += 0.05
        
        # Calculate variance and standard deviation
        for sentence in sentences:
            words_in_sentence = word_tokenize(sentence)
            total_squared_diff += (len(words_in_sentence) - total_average) ** 2
            if -3 <= len(words_in_sentence) - total_average <= 3 and avg_adverbs_per_sentence < 0.5:
                qualifying_sentences.append(sentence)
        
        total_variance = total_squared_diff / len(sentences)
        total_standard_deviation = math.sqrt(total_variance)
        
        # Check if the standard deviation exceeds a threshold
        if total_standard_deviation <= 5:
            avg_adverbs_per_sentence -= 0.3
            avg_adverbs_per_word -= 0.05
        
        final_sen_valsub = avg_adverbs_per_sentence
        final_word_valsub = avg_adverbs_per_word
        var = total_variance
        
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        
        pos_tags = hannanum.pos(text)
        
        # Count connecting adverbs
        connecting_adverb_counter = Counter()
        
        # Set for storing unique connecting adverbs
        unique_connecting_adverb_set = set()
        
        # Check for connecting adverbs using their POS tags
        for word, pos in pos_tags:
            if word in connecting_adverbs_list or pos == 'M':
                connecting_adverb_counter[word] += 1
                unique_connecting_adverb_set.add(word)
        
        # Calculate the average number of unique adverbs per sentence
        avg_adverbs_per_sentence2 = len(unique_connecting_adverb_set) / len(sentences)
        avg_adverbs_per_word2 = len(unique_connecting_adverb_set) / len(words)
        
        ad_num_sen = avg_adverbs_per_sentence2
        ad_num_word = avg_adverbs_per_word2
        
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        
        pos_tags = hannanum.pos(text)
        
        # Count connecting adverbs
        connecting_adverb_counter = Counter()
        
        # Set for storing unique connecting adverbs
        unique_connecting_adverb_set = set()
        
        # Check for connecting adverbs using their POS tags
        for word, pos in pos_tags:
            if word in connecting_adverbs_list or pos == 'M':
                connecting_adverb_counter[word] += 1
                unique_connecting_adverb_set.add(word)
        
        # Calculate the average number of unique adverbs per sentence
        avg_adverbs_per_sentence3 = len(unique_connecting_adverb_set) / len(sentences)
        avg_adverbs_per_word3 = len(unique_connecting_adverb_set) / len(words)
        
        total_squared_diff = 0
        total_average = len(words) / len(sentences)
        
        # Calculate adjusted averages
        if len(unique_connecting_adverb_set) >= 5 and avg_adverbs_per_sentence3 >= 1.0:
            avg_adverbs_per_sentence3 += 0.3
            avg_adverbs_per_word3 += 0.05
        
        # Calculate variance and standard deviation
        for sentence in sentences:
            words_in_sentence = word_tokenize(sentence)
            total_squared_diff += (len(words_in_sentence) - total_average) ** 2
        
        total_variance = total_squared_diff / len(sentences)
        total_standard_deviation = math.sqrt(total_variance)
        
        # Check if the standard deviation exceeds a threshold
        if total_standard_deviation >= 9:
            avg_adverbs_per_sentence3 += 0.3
            avg_adverbs_per_word3 += 0.05
        
        
        final_sen_valplus = avg_adverbs_per_sentence3
        final_word_valplus = avg_adverbs_per_word3
        
        def calculate_probability(sent_ap, next_word):
            input_text = sent_ap + " " + next_word
            input_ids = tokenizer.encode(input_text, return_tensors="tf")
            output = model(input_ids)
            logits = output.logits[0, -1, :]
            softmax_output = tf.nn.softmax(logits).numpy().tolist()
            next_word_id = input_ids[0, -1].numpy()
            return softmax_output[next_word_id]
        
        def calculate_sum(texting):
            sent_split = texting.split(" ")
        
            total_sum = 0
            sent_ap = ""
            for word in sent_split:
                next_word_probability = calculate_probability(sent_ap, word)
                sent_ap += " " + word
                total_sum += next_word_probability
        
            return round(total_sum * 1000 / len(sent_split), 5)
        
        file_sum = calculate_sum(text)
        probability = file_sum
        
        abc = [probability,classification,final_word_valplus,final_word_valsub,final_sen_valplus,final_sen_valsub,ad_num_word,ad_num_sen, var]
        for a, element in enumerate(abc):
            print(f"Element at index {a} is of type: {type(element)}")
        array = np.array(abc)
        array_reshaped = np.reshape(array, (1, 9))
        
        new_sample_scaled = scaler.transform(array_reshaped)
        
        probabilities = clf.predict_proba(new_sample_scaled)
        classlabel = clf.predict(new_sample_scaled)
        
        class_0_prob = probabilities[0, 0]
        class_1_prob = probabilities[0, 1]
        
        #print("Probability of class 0: {:.2f}".format(class_0_prob))
        #print("Probability of class 1: {:.2f}".format(class_1_prob))

        if(classlabel == 0):
            classlabel = '사람'
            pre_prop = np.round(class_0_prob * 100, 3)
        elif(classlabel == 1):
            classlabel = 'GPT'
            pre_prop = np.round(class_1_prob * 100, 3)


st.header("결과 : ")
if button:
    st.markdown('<style>h1{font-size: 80px;}</style>', unsafe_allow_html=True)
    st.write('결과는 ', classlabel, '의 텍스트이이며, ', classlabel, '일 확률은 ', str(pre_prop), '% 입니다.')
    if not qualifying_sentences:
        st.write(' ')
    else:
        st.write('gpt로 의심되는 문장은 : ')
        for sentence in qualifying_sentences:
            st.markdown(f"<p style='text-align: center; color: purple; font-size: 20px;'>{sentence}</p>", unsafe_allow_html=True)
